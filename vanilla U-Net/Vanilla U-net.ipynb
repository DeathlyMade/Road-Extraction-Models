{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b86374-7573-4cc4-a2c8-1b59670f4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52fd3acc-b87c-4ed5-937f-fc99a23b5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "# from data import trainGenerator, testGenerator, saveResult, testGenerator2\n",
    "import keras.backend as K\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd2330-c41f-4bfb-8ee6-854814c6d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# Directory containing the training data\n",
    "source_directory = '../DeepGlobe_road_extraction/train'   \n",
    "\n",
    "# Directory where PNG images will be moved\n",
    "satelliteDir = \"satellite_images/\"\n",
    "maskedDir = \"masked_images/\" \n",
    "\n",
    "# Create the subdirectory if it doesn't exist\n",
    "if not os.path.exists(satelliteDir):\n",
    "    os.makedirs(satelliteDir)\n",
    "    \n",
    "if not os.path.exists(maskedDir):\n",
    "    os.makedirs(maskedDir)\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(source_directory)\n",
    "\n",
    "# Filter and move PNG files to the subdirectory\n",
    "for file in all_files:\n",
    "    if file.endswith('.png'):\n",
    "        # Full path of the source file\n",
    "        source_path = os.path.join(source_directory, file)\n",
    "        \n",
    "        # Full path of the destination file\n",
    "        destination_path = os.path.join(maskedDir, file)\n",
    "        \n",
    "        # Move the file to the subdirectory\n",
    "        shutil.copy(source_path, destination_path)\n",
    "    elif file.endswith('.jpg'):\n",
    "        # Full path of the source file\n",
    "        source_path = os.path.join(source_directory, file)\n",
    "        \n",
    "        # Full path of the destination file\n",
    "        destination_path = os.path.join(satelliteDir, file)\n",
    "        \n",
    "        # Move the file to the subdirectory\n",
    "        shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82641da5-44a9-44bb-873b-f7374e86bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceDir_sat=\"./satellite_images/\"\n",
    "sourceDir_mask=\"./masked_images/\"\n",
    "trainDir_sat=\"./trainData/sat/\"\n",
    "valDir_sat=\"./valData/sat/\"\n",
    "trainDir_mask=\"./trainData/mask/\"\n",
    "valDir_mask=\"./valData/mask/\"\n",
    "#taking 600 images to train\n",
    "if not os.path.exists(trainDir_sat):\n",
    "    os.makedirs(trainDir_sat)\n",
    "\n",
    "if not os.path.exists(valDir_sat):\n",
    "    os.makedirs(valDir_sat)\n",
    "\n",
    "if not os.path.exists(trainDir_mask):\n",
    "    os.makedirs(trainDir_mask)\n",
    "\n",
    "if not os.path.exists(valDir_mask):\n",
    "    os.makedirs(valDir_mask)\n",
    "\n",
    "\n",
    "count=0\n",
    "#train data (satellite)\n",
    "for filename in os.listdir(sourceDir_sat):\n",
    "    if(count>=600): \n",
    "        break\n",
    "    if filename.lower().endswith('.jpg'):\n",
    "        source_file = os.path.join(sourceDir_sat, filename)\n",
    "        destination_file = os.path.join(trainDir_sat, filename)\n",
    "        \n",
    "        # Move the file\n",
    "        shutil.move(source_file, destination_file)\n",
    "        count+=1\n",
    "\n",
    "count=0\n",
    "#train data (masked)\n",
    "for filename in os.listdir(sourceDir_mask):\n",
    "    if(count>=600):\n",
    "        break\n",
    "    if filename.lower().endswith('.png'):\n",
    "        source_file = os.path.join(sourceDir_mask, filename)\n",
    "        destination_file = os.path.join(trainDir_mask, filename)\n",
    "        \n",
    "        # Move the file\n",
    "        shutil.move(source_file, destination_file)\n",
    "        count+=1\n",
    "\n",
    "count=0\n",
    "#val data(satellite)\n",
    "for filename in os.listdir(sourceDir_sat):\n",
    "    if(count>=50):\n",
    "        break\n",
    "    if filename.lower().endswith('.jpg'):\n",
    "        source_file = os.path.join(sourceDir_sat, filename)\n",
    "        destination_file = os.path.join(valDir_sat, filename)\n",
    "        \n",
    "        # Move the file\n",
    "        shutil.move(source_file, destination_file)\n",
    "        count+=1\n",
    "\n",
    "count=0\n",
    "#val data (mask)\n",
    "for filename in os.listdir(sourceDir_mask):\n",
    "    if(count>=50):\n",
    "        break\n",
    "    if filename.lower().endswith('.png'):\n",
    "        source_file = os.path.join(sourceDir_mask, filename)\n",
    "        destination_file = os.path.join(valDir_mask, filename)\n",
    "        \n",
    "        # Move the file\n",
    "        shutil.move(source_file, destination_file)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61687522-6fe2-40b8-acb5-19dee9c7079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to crop images to specific size\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "def crop_image(src, save_path):\n",
    "    TEST_SET = os.listdir(src)\n",
    "    img_h = 256\n",
    "    img_w = 256\n",
    "    stride = img_h-40\n",
    "    for n in range(len(TEST_SET)):\n",
    "        image_name = TEST_SET[n]\n",
    "        path1 = image_name[0:-7]+'mask.png'  #rename mask\n",
    "        # load the image\n",
    "        # image = cv2.imread(os.path.join(src,image_name), cv2.IMREAD_UNCHANGED)\n",
    "        # image = cv2.imread(os.path.join(src,image_name))\n",
    "        print(os.path.join(src,image_name))\n",
    "        image = io.imread(os.path.join(src,image_name))\n",
    "        \n",
    "        print(image.shape)\n",
    "        h, w, _ = image.shape\n",
    "        # h, w = image.shape\n",
    "\n",
    "        num = 0;\n",
    "        #image = img_to_array(image)\n",
    "        # padding_img = (padding_img - np.min(padding_img)) / (np.max(padding_img) - np.min(padding_img))\n",
    "\n",
    "        print('[{}/{}], croping:{}'.format(n+1, len(TEST_SET), image_name))\n",
    "\n",
    "        #mask_whole = np.zeros((h, w, 1), dtype=np.uint8)\n",
    "        #temp = np.zeros((img_h, img_h), dtype=np.uint8)\n",
    "\n",
    "        for i in range(0, (h // stride)+1):\n",
    "            for j in range(0, (w // stride)+1):\n",
    "                h_begin = i * stride\n",
    "                w_begin = j * stride\n",
    "                \n",
    "                if h_begin + img_h > h:\n",
    "                    h_begin = h_begin - (h_begin + img_h - h)\n",
    "                \n",
    "                if w_begin + img_w > w:\n",
    "                    w_begin = w_begin - (w_begin + img_w - w)\n",
    "                \n",
    "                crop = image[h_begin:h_begin + img_h, w_begin:w_begin + img_w] \n",
    "                if num <= 9:\n",
    "                    #path1 = image_name[0:-4]+'0'+ str(num)+'.jpg'\n",
    "                    path1 = image_name[0:-4]+'0'+ str(num)+'.png'\n",
    "                else:\n",
    "                    #path1 = image_name[0:-4]+str(num)+'.jpg'\n",
    "                    path1 = image_name[0:-4]+str(num)+'.png'\n",
    "                \n",
    "                #io.imsave(save_path + path1, crop)\n",
    "                cv2.imwrite(save_path + path1, crop)\n",
    "                num = num + 1\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a26f5-3887-4295-97e4-bc446898a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping Train Images\n",
    "trainDir_sat=\"./trainData/sat/\"\n",
    "valDir_sat=\"./valData/sat/\"\n",
    "trainDir_mask=\"./trainData/mask/\"\n",
    "valDir_mask=\"./valData/mask/\"\n",
    "\n",
    "cropsatDir_train = \"./trainData/cropped_sat/\"\n",
    "cropmaskDir_train = \"./trainData/cropped_mask/\" \n",
    "\n",
    "cropsatDir_val = \"./valData/cropped_sat/\"\n",
    "cropmaskDir_val = \"./valData/cropped_mask/\" \n",
    "\n",
    "if not os.path.exists(cropsatDir_train):\n",
    "    os.makedirs(cropsatDir_train)\n",
    "    \n",
    "if not os.path.exists(cropmaskDir_train):\n",
    "    os.makedirs(cropmaskDir_train)\n",
    "\n",
    "if not os.path.exists(cropsatDir_val):\n",
    "    os.makedirs(cropsatDir_val)\n",
    "    \n",
    "if not os.path.exists(cropmaskDir_val):\n",
    "    os.makedirs(cropmaskDir_val)\n",
    "\n",
    "crop_image(trainDir_sat, cropsatDir_train)\n",
    "crop_image(trainDir_mask, cropmaskDir_train)\n",
    "crop_image(valDir_sat, cropsatDir_val)\n",
    "crop_image(valDir_mask, cropmaskDir_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d808cc30-448b-462c-86a6-c2ec788c31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cef9dc9-1c02-462c-bbd7-2f36c708f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(inputs, num_filters): \n",
    "    # Convolution with 3x3 filter followed by ReLU activation \n",
    "    x = tf.keras.layers.Conv2D(num_filters, 3,padding = 'same')(inputs) \n",
    "    x = tf.keras.layers.Activation('relu')(x) \n",
    "\n",
    "    # Convolution with 3x3 filter followed by ReLU activation \n",
    "    x = tf.keras.layers.Conv2D(num_filters, 3, padding = 'same')(x) \n",
    "    x = tf.keras.layers.Activation('relu')(x) \n",
    "\n",
    "    # Max Pooling with 2x2 filter \n",
    "    x = tf.keras.layers.MaxPool2D(pool_size = (2, 2), strides = 2)(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c4ed7a-f0f1-46b3-9e2c-f834c2bc79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(inputs, skip_features, num_filters): \n",
    "    # Upsampling with 2x2 filter \n",
    "    x = tf.keras.layers.Conv2DTranspose(num_filters, (2, 2), strides = 2, padding = 'same')(inputs) \n",
    "      \n",
    "    # Copy and crop the skip features  \n",
    "    # to match the shape of the upsampled input \n",
    "    skip_features = tf.image.resize(skip_features, size = (x.shape[1], x.shape[2])) \n",
    "    x = tf.keras.layers.Concatenate()([x, skip_features]) \n",
    "      \n",
    "    # Convolution with 3x3 filter followed by ReLU activation \n",
    "    x = tf.keras.layers.Conv2D(num_filters, 3, padding = 'same')(x) \n",
    "    x = tf.keras.layers.Activation('relu')(x) \n",
    "  \n",
    "    # Convolution with 3x3 filter followed by ReLU activation \n",
    "    x = tf.keras.layers.Conv2D(num_filters, 3, padding = 'same')(x) \n",
    "    x = tf.keras.layers.Activation('relu')(x) \n",
    "      \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e7ee76-cd70-4651-89e1-d7eec1377545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    #y_true_f = np.array(K.flatten(y_true))\n",
    "    #y_pred_f = np.array(K.flatten(y_pred))\n",
    "    #y_pred_f[y_pred_f >= 0.5] = 1\n",
    "    #y_pred_f[y_pred_f < 0.5] = 0\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "\n",
    "def final_loss(y_true, y_pred):\n",
    "    loss1 = binary_crossentropy(y_true, y_pred)\n",
    "    loss2 = 1 - IoU(y_true, y_pred)\n",
    "    return loss1 + loss2\n",
    "def unet(pretrained_weights = None,input_size = (256,256,3)):\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same')(conv4)\n",
    "    #drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same')(conv5)\n",
    "    #drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv5))\n",
    "    merge6 = concatenate([conv4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 2e-4), loss = final_loss, metrics = [IoU])\n",
    "    #model.compile(optimizer = SGD(lr=1e-3, decay=0.0, momentum=0.9, nesterov=True), loss = final_loss, metrics = [IoU])\n",
    "    #model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b8155a-07f9-4b7d-847a-e2a562569367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"rgb\",\n",
    "#                     mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "#                     flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1):\n",
    "#     '''\n",
    "#     can generate image and mask at the same time\n",
    "#     use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "#     if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "#     '''\n",
    "#     image_datagen = ImageDataGenerator(**aug_dict)\n",
    "#     mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "#     image_generator = image_datagen.flow_from_directory(\n",
    "#         train_path,\n",
    "#         classes = [image_folder],\n",
    "#         class_mode = None,\n",
    "#         color_mode = image_color_mode,\n",
    "#         target_size = target_size,\n",
    "#         batch_size = batch_size,\n",
    "#         save_to_dir = save_to_dir,\n",
    "#         save_prefix  = image_save_prefix,\n",
    "#         seed = seed)\n",
    "#     mask_generator = mask_datagen.flow_from_directory(\n",
    "#         train_path,\n",
    "#         classes = [mask_folder],\n",
    "#         class_mode = None,\n",
    "#         color_mode = mask_color_mode,\n",
    "#         target_size = target_size,\n",
    "#         batch_size = batch_size,\n",
    "#         save_to_dir = save_to_dir,\n",
    "#         save_prefix  = mask_save_prefix,\n",
    "#         seed = seed)\n",
    "#     train_generator = zip(image_generator, mask_generator)\n",
    "#     for (img,mask) in train_generator:\n",
    "#         #img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
    "#         img = img / 255.0\n",
    "#         mask = mask / 255.0\n",
    "#         yield (img,mask)\n",
    "\n",
    "\n",
    "def trainGenerator(batch_size, train_path, image_folder, mask_folder, aug_dict, image_color_mode=\"rgb\",\n",
    "                    mask_color_mode=\"grayscale\", image_save_prefix=\"image\", mask_save_prefix=\"mask\",\n",
    "                    flag_multi_class=False, num_class=2, save_to_dir=None, target_size=(256, 256), seed=1):\n",
    "    '''\n",
    "    Can generate image and mask at the same time.\n",
    "    Uses the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same.\n",
    "    If you want to visualize the results of generator, set save_to_dir = \"your path\".\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes=[image_folder],\n",
    "        class_mode=None,\n",
    "        color_mode=image_color_mode,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        save_to_dir=save_to_dir,\n",
    "        save_prefix=image_save_prefix,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes=[mask_folder],\n",
    "        class_mode=None,\n",
    "        color_mode=mask_color_mode,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        save_to_dir=save_to_dir,\n",
    "        save_prefix=mask_save_prefix,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Check if generators produce the same number of samples\n",
    "    if image_generator.batch_size != mask_generator.batch_size:\n",
    "        raise ValueError(\"Batch sizes of image_generator and mask_generator must be equal.\")\n",
    "\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img, mask) in train_generator:\n",
    "        if img.shape[0] != mask.shape[0]:\n",
    "            raise ValueError(\"Batch sizes of images and masks do not match.\")\n",
    "        \n",
    "        # Normalize images and masks\n",
    "        img = img / 255.0\n",
    "        mask = mask / 255.0\n",
    "        \n",
    "        yield (img, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c364d3c-2da8-4c60-84b9-c41b38434ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df949ed2-eccd-4e83-98d2-035195cd311a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "data_gen_args = dict(rotation_range=90.,\n",
    "                    #width_shift_range=0.1,\n",
    "                    #height_shift_range=0.1,\n",
    "                    #shear_range=0.1,\n",
    "                    #zoom_range=0.1,\n",
    "                    fill_mode='nearest',\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor=0.2, patience=3, verbose=0, mode='min', epsilon=1e-4, \n",
    "                              cooldown=0, min_lr=1e-6)\n",
    "visual = TensorBoard(log_dir='./VanillaUNet_log', histogram_freq=0, write_graph=True, write_images=True)\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=7, verbose=0, mode='min')\n",
    "\n",
    "train_Gene = trainGenerator(1,'./trainData/','cropped_sat','cropped_mask',data_gen_args,save_to_dir = None)\n",
    "# to be fixed(train-val split)\n",
    "val_Gene = trainGenerator(1,'./valData/','cropped_sat','cropped_mask',data_gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "040f123a-8b3f-4278-9efc-205e7ec077d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8167793b-3726-4914-acf0-c997dc226911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.7821 - IoU: 0.0410\n",
      "Epoch 1: val_loss improved from inf to 1.21658, saving model to U-Net.hdf5\n",
      "50/50 [==============================] - 16s 306ms/step - loss: 1.7821 - IoU: 0.0410 - val_loss: 1.2166 - val_IoU: 0.0389 - lr: 2.0000e-04\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.2289 - IoU: 0.0437\n",
      "Epoch 2: val_loss improved from 1.21658 to 1.15329, saving model to U-Net.hdf5\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 1.2289 - IoU: 0.0437 - val_loss: 1.1533 - val_IoU: 0.0240 - lr: 2.0000e-04\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1619 - IoU: 0.0256\n",
      "Epoch 3: val_loss improved from 1.15329 to 1.14708, saving model to U-Net.hdf5\n",
      "50/50 [==============================] - 20s 408ms/step - loss: 1.1619 - IoU: 0.0256 - val_loss: 1.1471 - val_IoU: 0.0141 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1473 - IoU: 0.0172\n",
      "Epoch 4: val_loss did not improve from 1.14708\n",
      "50/50 [==============================] - 19s 392ms/step - loss: 1.1473 - IoU: 0.0172 - val_loss: 1.1528 - val_IoU: 0.0247 - lr: 2.0000e-04\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1639 - IoU: 0.0294\n",
      "Epoch 5: val_loss did not improve from 1.14708\n",
      "50/50 [==============================] - 18s 358ms/step - loss: 1.1639 - IoU: 0.0294 - val_loss: 1.1634 - val_IoU: 0.0362 - lr: 2.0000e-04\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1548 - IoU: 0.0278\n",
      "Epoch 6: val_loss improved from 1.14708 to 1.13542, saving model to U-Net.hdf5\n",
      "50/50 [==============================] - 18s 360ms/step - loss: 1.1548 - IoU: 0.0278 - val_loss: 1.1354 - val_IoU: 0.0329 - lr: 2.0000e-04\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1159 - IoU: 0.0317\n",
      "Epoch 7: val_loss did not improve from 1.13542\n",
      "50/50 [==============================] - 14s 288ms/step - loss: 1.1159 - IoU: 0.0317 - val_loss: 1.1539 - val_IoU: 0.0189 - lr: 2.0000e-04\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1524 - IoU: 0.0544\n",
      "Epoch 8: val_loss improved from 1.13542 to 1.10888, saving model to U-Net.hdf5\n",
      "50/50 [==============================] - 15s 297ms/step - loss: 1.1524 - IoU: 0.0544 - val_loss: 1.1089 - val_IoU: 0.0368 - lr: 2.0000e-04\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1396 - IoU: 0.0621\n",
      "Epoch 9: val_loss did not improve from 1.10888\n",
      "50/50 [==============================] - 14s 286ms/step - loss: 1.1396 - IoU: 0.0621 - val_loss: 1.1368 - val_IoU: 0.0705 - lr: 2.0000e-04\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1744 - IoU: 0.0736\n",
      "Epoch 10: val_loss did not improve from 1.10888\n",
      "50/50 [==============================] - 14s 286ms/step - loss: 1.1744 - IoU: 0.0736 - val_loss: 1.1467 - val_IoU: 0.0345 - lr: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x223345ddb50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint('U-Net.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n",
    "model.fit(train_Gene,steps_per_epoch=50,epochs=10,\n",
    "                    callbacks=[model_checkpoint, visual, reduce_lr, earlystop], \n",
    "                    validation_data=val_Gene, validation_steps=220)#step_per_epoch and validation_steps equals to number of samples divide batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14a57c30-d27e-46bc-a245-b587fea3b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_z(src, predict_path):\n",
    "    TEST_SET = os.listdir(src)\n",
    "    model = unet()\n",
    "    print('Loading Model weights...')\n",
    "    model.load_weights('U-Net.hdf5')\n",
    "    print('Completed!')\n",
    "    img_h = 256\n",
    "    img_w = 256\n",
    "    stride = img_h - 40\n",
    "\n",
    "    for n in range(len(TEST_SET)):\n",
    "        path = TEST_SET[n]\n",
    "        path1 = path[0:-7] + 'mask.png'  # Rename mask\n",
    "        image = io.imread(os.path.join(src, path))\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        image = image / 255.0\n",
    "        print('[{}/{}], predicting: {}'.format(n + 1, len(TEST_SET), path))\n",
    "\n",
    "        mask_whole = np.zeros((h, w, 1), dtype=np.uint8)\n",
    "\n",
    "        for i in range(0, (h // stride) + 1):\n",
    "            for j in range(0, (w // stride) + 1):\n",
    "                h_begin = i * stride\n",
    "                w_begin = j * stride\n",
    "\n",
    "                if h_begin + img_h > h:\n",
    "                    h_begin = h_begin - (h_begin + img_h - h)\n",
    "\n",
    "                if w_begin + img_w > w:\n",
    "                    w_begin = w_begin - (w_begin + img_w - w)\n",
    "\n",
    "                crop = image[h_begin:h_begin + img_h, w_begin:w_begin + img_w, :3]\n",
    "                ch, cw, _ = crop.shape\n",
    "\n",
    "                if ch != img_h or cw != img_w:\n",
    "                    print('Invalid size!')\n",
    "                    print(i, j, h_begin, w_begin, ch, cw)\n",
    "                    continue\n",
    "\n",
    "                crop = np.expand_dims(crop, axis=0)\n",
    "                pred = model.predict(crop, verbose=2)\n",
    "                pred = pred.reshape((img_h, img_w, 1)).astype(np.float64)\n",
    "\n",
    "                # Debugging\n",
    "                print('Prediction min:', np.min(pred))\n",
    "                print('Prediction max:', np.max(pred))\n",
    "\n",
    "                # Thresholding\n",
    "                pred[pred >= 0.15] = 1\n",
    "                pred[pred < 0.15] = 0\n",
    "                pred = pred * 255\n",
    "\n",
    "                # Update mask\n",
    "                mask_whole[h_begin:h_begin + img_h, w_begin:w_begin + img_w] = pred\n",
    "\n",
    "        # Ensure correct shape and dtype\n",
    "        mask_whole = np.squeeze(mask_whole)\n",
    "        mask_whole = mask_whole.astype(np.uint8)\n",
    "        \n",
    "        # Save mask\n",
    "        cv2.imwrite(predict_path + path1, mask_whole[0:h, 0:w])\n",
    "        print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f357510-724d-4b3a-8c33-036473c19b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model weights...\n",
      "Completed!\n",
      "[1/3], predicting: 58949_sat.jpg\n",
      "1/1 - 0s - 153ms/epoch - 153ms/step\n",
      "Prediction min: 0.002804800169542432\n",
      "Prediction max: 0.3678666353225708\n",
      "1/1 - 0s - 15ms/epoch - 15ms/step\n",
      "Prediction min: 0.0027017632964998484\n",
      "Prediction max: 0.3840187191963196\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0043958681635558605\n",
      "Prediction max: 0.35241827368736267\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0020188966300338507\n",
      "Prediction max: 0.34509846568107605\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0019965474493801594\n",
      "Prediction max: 0.3565346896648407\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0016886458033695817\n",
      "Prediction max: 0.343956857919693\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Prediction min: 0.001723895431496203\n",
      "Prediction max: 0.3440151810646057\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0030033860821276903\n",
      "Prediction max: 0.39681360125541687\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0020015856716781855\n",
      "Prediction max: 0.3269858658313751\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Prediction min: 0.0020015856716781855\n",
      "Prediction max: 0.3344259560108185\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "Prediction min: 0.0024690995924174786\n",
      "Prediction max: 0.34873589873313904\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0022042884957045317\n",
      "Prediction max: 0.3462219834327698\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0033227340318262577\n",
      "Prediction max: 0.34666451811790466\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.002897733822464943\n",
      "Prediction max: 0.3335150480270386\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0024597872979938984\n",
      "Prediction max: 0.34414780139923096\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0021380302496254444\n",
      "Prediction max: 0.3949733376502991\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.004477042704820633\n",
      "Prediction max: 0.3882562220096588\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.003504299558699131\n",
      "Prediction max: 0.3575109541416168\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Prediction min: 0.0022543747909367085\n",
      "Prediction max: 0.3347421884536743\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0017770916456356645\n",
      "Prediction max: 0.3433960974216461\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Prediction min: 0.0013774583349004388\n",
      "Prediction max: 0.39012762904167175\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.004586216527968645\n",
      "Prediction max: 0.3925420641899109\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.003504299558699131\n",
      "Prediction max: 0.3418934643268585\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "Prediction min: 0.002426448743790388\n",
      "Prediction max: 0.33578577637672424\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0017770916456356645\n",
      "Prediction max: 0.35120561718940735\n",
      "Done!\n",
      "[2/3], predicting: 62796_sat.jpg\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "Prediction min: 0.00038664331077598035\n",
      "Prediction max: 0.3797471225261688\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0001628739555599168\n",
      "Prediction max: 0.3738858699798584\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.00022768265625927597\n",
      "Prediction max: 0.326791912317276\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.00022768265625927597\n",
      "Prediction max: 0.3680747151374817\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.000985142425633967\n",
      "Prediction max: 0.39375919103622437\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0006865904433652759\n",
      "Prediction max: 0.3604387640953064\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0007330490625463426\n",
      "Prediction max: 0.3634997308254242\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.00018342236580792814\n",
      "Prediction max: 0.3805219233036041\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 4.256535248714499e-05\n",
      "Prediction max: 0.3578473925590515\n",
      "1/1 - 0s - 15ms/epoch - 15ms/step\n",
      "Prediction min: 8.782123768469319e-05\n",
      "Prediction max: 0.35734280943870544\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0005752810975536704\n",
      "Prediction max: 0.3155072331428528\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.00030829719617031515\n",
      "Prediction max: 0.36216282844543457\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Prediction min: 0.0005567501648329198\n",
      "Prediction max: 0.34341588616371155\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 8.782123768469319e-05\n",
      "Prediction max: 0.3645573556423187\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 8.782123768469319e-05\n",
      "Prediction max: 0.37193065881729126\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "Prediction min: 0.00035330167156644166\n",
      "Prediction max: 0.34588363766670227\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.00029426981927827\n",
      "Prediction max: 0.3411259651184082\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Prediction min: 0.0003691046149469912\n",
      "Prediction max: 0.33074042201042175\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0007435547304339707\n",
      "Prediction max: 0.35094794631004333\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0007150818128138781\n",
      "Prediction max: 0.35156235098838806\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.00020670765661634505\n",
      "Prediction max: 0.3448181450366974\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.00029426981927827\n",
      "Prediction max: 0.30682605504989624\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "Prediction min: 0.0002771435829345137\n",
      "Prediction max: 0.34813985228538513\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Prediction min: 0.00032617911347188056\n",
      "Prediction max: 0.35785147547721863\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0003001475415658206\n",
      "Prediction max: 0.3487948179244995\n",
      "Done!\n",
      "[3/3], predicting: 74091_sat.jpg\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0010133619653061032\n",
      "Prediction max: 0.3512256145477295\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0014635049737989902\n",
      "Prediction max: 0.3360912501811981\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0008818141068331897\n",
      "Prediction max: 0.3532083034515381\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0012082308530807495\n",
      "Prediction max: 0.3502351939678192\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0010918851476162672\n",
      "Prediction max: 0.3655136823654175\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0009534218697808683\n",
      "Prediction max: 0.336991548538208\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Prediction min: 0.0011601134901866317\n",
      "Prediction max: 0.35882827639579773\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0008145517786033452\n",
      "Prediction max: 0.3384723365306854\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0005134413950145245\n",
      "Prediction max: 0.3399267792701721\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0005127937765792012\n",
      "Prediction max: 0.36565300822257996\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.001190332812257111\n",
      "Prediction max: 0.3335764706134796\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0015465550823137164\n",
      "Prediction max: 0.36940550804138184\n",
      "1/1 - 0s - 15ms/epoch - 15ms/step\n",
      "Prediction min: 0.0011414868058636785\n",
      "Prediction max: 0.30420875549316406\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "Prediction min: 0.0004936026525683701\n",
      "Prediction max: 0.38310855627059937\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0004936026525683701\n",
      "Prediction max: 0.33069679141044617\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.001357056898996234\n",
      "Prediction max: 0.3557545244693756\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0012752041220664978\n",
      "Prediction max: 0.3592248260974884\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0006747095612809062\n",
      "Prediction max: 0.3373475968837738\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0009498205618001521\n",
      "Prediction max: 0.3632786273956299\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0009357468225061893\n",
      "Prediction max: 0.33757269382476807\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0011139101115986705\n",
      "Prediction max: 0.3667428493499756\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "Prediction min: 0.0013923991937190294\n",
      "Prediction max: 0.33957961201667786\n",
      "1/1 - 0s - 25ms/epoch - 25ms/step\n",
      "Prediction min: 0.0006747095612809062\n",
      "Prediction max: 0.32861465215682983\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0009876310359686613\n",
      "Prediction max: 0.38203611969947815\n",
      "1/1 - 0s - 16ms/epoch - 16ms/step\n",
      "Prediction min: 0.0009876310359686613\n",
      "Prediction max: 0.3745850622653961\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "fileDir = \"./test/\"  #test images(1024*1024)\n",
    "preDir = \"./predicted/\" #Dir of predict mask\n",
    "predict_z(fileDir, preDir)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5144f39c-3564-4e14-b7b6-b528f2c534b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
