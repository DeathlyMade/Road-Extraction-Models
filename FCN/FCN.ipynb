{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e541527b-3e61-4c8c-bc2a-432f19ec5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c47e54-ddac-4312-9682-6e8fd3a59d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from model import unet, segnet_vgg16, fcn_vgg16_8s, VGGUnet2, D_resunet, D_resunet1\n",
    "from data import trainGenerator, testGenerator, saveResult, testGenerator2\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "import keras.backend as K\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f303e672-abff-45ef-8e70-aff7eab9cb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/membrane/IEEE_road/train/sat_img/104_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[1/50], croping:104_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/1054_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[2/50], croping:1054_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/113_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[3/50], croping:113_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/1849_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[4/50], croping:1849_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/1934_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[5/50], croping:1934_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/1945_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[6/50], croping:1945_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/2129_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[7/50], croping:2129_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/2262_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[8/50], croping:2262_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/2495_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[9/50], croping:2495_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/2647_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[10/50], croping:2647_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/2667_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[11/50], croping:2667_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/2730_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[12/50], croping:2730_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/3280_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[13/50], croping:3280_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/343_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[14/50], croping:343_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/3452_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[15/50], croping:3452_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/3530_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[16/50], croping:3530_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/3573_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[17/50], croping:3573_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/3594_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[18/50], croping:3594_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/3602_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[19/50], croping:3602_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/388_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[20/50], croping:388_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/4131_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[21/50], croping:4131_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/4163_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[22/50], croping:4163_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/4209_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[23/50], croping:4209_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/4227_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[24/50], croping:4227_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/4301_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[25/50], croping:4301_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/4903_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[26/50], croping:4903_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/5279_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[27/50], croping:5279_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/5386_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[28/50], croping:5386_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/5415_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[29/50], croping:5415_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/541_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[30/50], croping:541_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/5501_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[31/50], croping:5501_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/5523_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[32/50], croping:5523_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/5555_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[33/50], croping:5555_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/562_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[34/50], croping:562_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/5668_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[35/50], croping:5668_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/5740_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[36/50], croping:5740_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/5947_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[37/50], croping:5947_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/602_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[38/50], croping:602_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/627_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[39/50], croping:627_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/6281_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[40/50], croping:6281_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/6294_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[41/50], croping:6294_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/6383_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[42/50], croping:6383_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/6415_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[43/50], croping:6415_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/655_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[44/50], croping:655_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/6696_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[45/50], croping:6696_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/6822_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[46/50], croping:6822_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/7011_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[47/50], croping:7011_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/7046_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[48/50], croping:7046_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/7372_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[49/50], croping:7372_sat.jpg\n",
      "Done!\n",
      "data/membrane/IEEE_road/train/sat_img/951_sat.jpg\n",
      "(1024, 1024, 3)\n",
      "[50/50], croping:951_sat.jpg\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jan 23 17:07:33 2019\n",
    "Note! crop the images and masks need to change several lines code(60/63)\n",
    "@author: zetn\n",
    "\"\"\"\n",
    "from model import unet, segnet_vgg16, fcn_vgg16_8s, VGGUnet2, res_unet, D_resunet1\n",
    "from data import trainGenerator, testGenerator, saveResult, testGenerator2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "\n",
    "#fileDir = \"data/membrane/test/sub_test/mask8\"  #test images(1024*1024)\n",
    "fileDir = \"data/membrane/IEEE_road/train/sat_img/\"\n",
    "preDir = \"data/membrane/IEEE_road/train/sat_img_crops/\" #Dir of predict mask\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "def crop_image(src, save_path):\n",
    "    TEST_SET = os.listdir(src)\n",
    "    img_h = 256\n",
    "    img_w = 256\n",
    "    stride = img_h-40\n",
    "    for n in range(len(TEST_SET)):\n",
    "        image_name = TEST_SET[n]\n",
    "        path1 = image_name[0:-7]+'mask.png'  #rename mask\n",
    "        # load the image\n",
    "        # image = cv2.imread(os.path.join(src,image_name), cv2.IMREAD_UNCHANGED)\n",
    "        # image = cv2.imread(os.path.join(src,image_name))\n",
    "        print(os.path.join(src,image_name))\n",
    "        image = io.imread(os.path.join(src,image_name))\n",
    "        \n",
    "        print(image.shape)\n",
    "        h, w, _ = image.shape\n",
    "        # h, w = image.shape\n",
    "\n",
    "        num = 0;\n",
    "        #image = img_to_array(image)\n",
    "        # padding_img = (padding_img - np.min(padding_img)) / (np.max(padding_img) - np.min(padding_img))\n",
    "\n",
    "        print('[{}/{}], croping:{}'.format(n+1, len(TEST_SET), image_name))\n",
    "\n",
    "        #mask_whole = np.zeros((h, w, 1), dtype=np.uint8)\n",
    "        #temp = np.zeros((img_h, img_h), dtype=np.uint8)\n",
    "\n",
    "        for i in range(0, (h // stride)+1):\n",
    "            for j in range(0, (w // stride)+1):\n",
    "                h_begin = i * stride\n",
    "                w_begin = j * stride\n",
    "                \n",
    "                if h_begin + img_h > h:\n",
    "                    h_begin = h_begin - (h_begin + img_h - h)\n",
    "                \n",
    "                if w_begin + img_w > w:\n",
    "                    w_begin = w_begin - (w_begin + img_w - w)\n",
    "                \n",
    "                crop = image[h_begin:h_begin + img_h, w_begin:w_begin + img_w] \n",
    "                if num <= 9:\n",
    "                    #path1 = image_name[0:-4]+'0'+ str(num)+'.jpg'\n",
    "                    path1 = image_name[0:-4]+'0'+ str(num)+'.png'\n",
    "                else:\n",
    "                    #path1 = image_name[0:-4]+str(num)+'.jpg'\n",
    "                    path1 = image_name[0:-4]+str(num)+'.png'\n",
    "                \n",
    "                #io.imsave(save_path + path1, crop)\n",
    "                cv2.imwrite(save_path + path1, crop)\n",
    "                num = num + 1\n",
    "        print('Done!')\n",
    "crop_image(fileDir, preDir)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "346285ab-a965-43ef-b87f-ada381639b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Found 1250 images belonging to 1 classes.\n",
      "Found 1250 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\subha\\AppData\\Local\\Temp\\ipykernel_46236\\2489772996.py:29: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_Gene,steps_per_epoch=50,epochs=10,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1772 - IoU: 0.0955Found 375 images belonging to 1 classes.\n",
      "Found 375 images belonging to 1 classes.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.33775, saving model to D_Resunet.hdf5\n",
      "50/50 [==============================] - 82s 2s/step - loss: 1.1772 - IoU: 0.0955 - val_loss: 1.3377 - val_IoU: 0.0260 - lr: 2.0000e-04\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1136 - IoU: 0.0663\n",
      "Epoch 2: val_loss improved from 1.33775 to 1.14118, saving model to D_Resunet.hdf5\n",
      "50/50 [==============================] - 100s 2s/step - loss: 1.1136 - IoU: 0.0663 - val_loss: 1.1412 - val_IoU: 0.0638 - lr: 2.0000e-04\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1025 - IoU: 0.0926\n",
      "Epoch 3: val_loss improved from 1.14118 to 1.13564, saving model to D_Resunet.hdf5\n",
      "50/50 [==============================] - 98s 2s/step - loss: 1.1025 - IoU: 0.0926 - val_loss: 1.1356 - val_IoU: 0.0277 - lr: 2.0000e-04\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0581 - IoU: 0.1205\n",
      "Epoch 4: val_loss improved from 1.13564 to 1.12379, saving model to D_Resunet.hdf5\n",
      "50/50 [==============================] - 88s 2s/step - loss: 1.0581 - IoU: 0.1205 - val_loss: 1.1238 - val_IoU: 0.0385 - lr: 2.0000e-04\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0967 - IoU: 0.0829\n",
      "Epoch 5: val_loss did not improve from 1.12379\n",
      "50/50 [==============================] - 84s 2s/step - loss: 1.0967 - IoU: 0.0829 - val_loss: 1.1414 - val_IoU: 0.0301 - lr: 2.0000e-04\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1053 - IoU: 0.0687\n",
      "Epoch 6: val_loss improved from 1.12379 to 1.10374, saving model to D_Resunet.hdf5\n",
      "50/50 [==============================] - 85s 2s/step - loss: 1.1053 - IoU: 0.0687 - val_loss: 1.1037 - val_IoU: 0.0497 - lr: 2.0000e-04\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1014 - IoU: 0.0836\n",
      "Epoch 7: val_loss improved from 1.10374 to 1.08786, saving model to D_Resunet.hdf5\n",
      "50/50 [==============================] - 86s 2s/step - loss: 1.1014 - IoU: 0.0836 - val_loss: 1.0879 - val_IoU: 0.0663 - lr: 2.0000e-04\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0768 - IoU: 0.0745\n",
      "Epoch 8: val_loss improved from 1.08786 to 1.08765, saving model to D_Resunet.hdf5\n",
      "50/50 [==============================] - 96s 2s/step - loss: 1.0768 - IoU: 0.0745 - val_loss: 1.0876 - val_IoU: 0.0681 - lr: 2.0000e-04\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0838 - IoU: 0.1424\n",
      "Epoch 9: val_loss did not improve from 1.08765\n",
      "50/50 [==============================] - 80s 2s/step - loss: 1.0838 - IoU: 0.1424 - val_loss: 1.2403 - val_IoU: 0.0966 - lr: 2.0000e-04\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0041 - IoU: 0.1418\n",
      "Epoch 10: val_loss did not improve from 1.08765\n",
      "50/50 [==============================] - 48s 970ms/step - loss: 1.0041 - IoU: 0.1418 - val_loss: 1.3051 - val_IoU: 0.0458 - lr: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1963d3ade20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen_args = dict(rotation_range=90.,\n",
    "                    #width_shift_range=0.1,\n",
    "                    #height_shift_range=0.1,\n",
    "                    #shear_range=0.1,\n",
    "                    #zoom_range=0.1,\n",
    "                    fill_mode='nearest',\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True)\n",
    "\n",
    "train_Gene = trainGenerator(2,'data/membrane/IEEE_road/train/','sat_img_crops','masks_crops',data_gen_args,save_to_dir = None)\n",
    "val_Gene = trainGenerator(2,'data/membrane/IEEE_road/test/','sat_img_crops','masks_crops',data_gen_args)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor=0.2, patience=3, verbose=0, mode='min', epsilon=1e-4, \n",
    "                              cooldown=0, min_lr=1e-6)\n",
    "visual = TensorBoard(log_dir='./D_resunet1_log', histogram_freq=0, write_graph=True, write_images=True)\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=7, verbose=0, mode='min')\n",
    "#model = unet()\n",
    "#model = segnet_vgg16()\n",
    "#model = fcn_vgg16_8s()\n",
    "#model.load_weights('fcn_vgg16_8s.hdf5')\n",
    "#model = fcn_vgg16_8s()\n",
    "#model = VGGUnet2()\n",
    "model = D_resunet()\n",
    "\n",
    "#model = res_unet1()\n",
    "#model.load_weights('res_unet.hdf5')\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('D_Resunet.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_Gene,steps_per_epoch=50,epochs=10,\n",
    "                    callbacks=[model_checkpoint, visual, reduce_lr, earlystop], \n",
    "                    validation_data=val_Gene, validation_steps=220)#step_per_epoch and validation_steps equals to number of samples divide batchsize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6595ca-ac5e-494d-8be2-ca02e18f283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model weights...\n",
      "completed!\n",
      "[1/4], predicting:355_sat.jpg\n",
      "1/1 - 0s - 399ms/epoch - 399ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 23ms/epoch - 23ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "[2/4], predicting:48631_sat.jpg\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 22ms/epoch - 22ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "[3/4], predicting:49663_sat.jpg\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "[4/4], predicting:9156_sat.jpg\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 22ms/epoch - 22ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 21ms/epoch - 21ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 20ms/epoch - 20ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n",
      "1/1 - 0s - 19ms/epoch - 19ms/step\n",
      "1/1 - 0s - 17ms/epoch - 17ms/step\n",
      "1/1 - 0s - 18ms/epoch - 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan 11 12:12:34 2019\n",
    "带重叠的滑动窗口patch（256*256）预测并缝合形成大的mask（1024*1024）图片\n",
    "@author: zetn\n",
    "\"\"\"\n",
    "\n",
    "from model import unet, segnet_vgg16, fcn_vgg16_8s, VGGUnet2, res_unet, D_resunet1\n",
    "from data import trainGenerator, testGenerator, saveResult, testGenerator2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import os, cv2\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "\n",
    "fileDir = \"data/membrane/IEEE_road/test/test_images/\"  #test images(1024*1024)\n",
    "#fileDir = \"data/membrane/train/f\"\n",
    "preDir = \"data/membrane/IEEE_road/test/sub_test/predict/\" #Dir of predict mask\n",
    "\n",
    "\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "def predict_z(src, predict_path):\n",
    "    TEST_SET = os.listdir(src)\n",
    "    model = D_resunet()\n",
    "    #model = res_unet1()\n",
    "    print('Loading Model weights...')\n",
    "    model.load_weights('D_resunet.hdf5')\n",
    "    print('completed!')\n",
    "    img_h = 256\n",
    "    img_w = 256\n",
    "    stride = img_h-16\n",
    "    for n in range(len(TEST_SET)):\n",
    "        path = TEST_SET[n]\n",
    "        path1 = path[0:-7]+'mask.png'  #rename mask\n",
    "        # load the image\n",
    "        image = io.imread(os.path.join(src,path))\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        image = image / 255.0\n",
    "        #image = img_to_array(image)\n",
    "        # padding_img = (padding_img - np.min(padding_img)) / (np.max(padding_img) - np.min(padding_img))\n",
    "\n",
    "        print('[{}/{}], predicting:{}'.format(n+1, len(TEST_SET), path))\n",
    "\n",
    "        mask_whole = np.zeros((h, w, 1), dtype=np.uint8)\n",
    "        #temp = np.zeros((img_h, img_h), dtype=np.uint8)\n",
    "\n",
    "        for i in range(0, (h // stride)+1):\n",
    "            for j in range(0, (w // stride)+1):\n",
    "                h_begin = i * stride\n",
    "                w_begin = j * stride\n",
    "                \n",
    "                if h_begin + img_h > h:\n",
    "                    h_begin = h_begin - (h_begin + img_h - h)\n",
    "                \n",
    "                if w_begin + img_w > w:\n",
    "                    w_begin = w_begin - (w_begin + img_w - w)\n",
    "                \n",
    "                crop = image[h_begin:h_begin + img_h, w_begin:w_begin + img_w, :3] #[****)\n",
    "                \n",
    "                ch, cw, _ = crop.shape\n",
    "\n",
    "                if ch != img_h or cw != img_h:\n",
    "                    print('invalid size!')\n",
    "                    print(i, j, h_begin, w_begin, ch, cw)\n",
    "                    break\n",
    "                \n",
    "                crop = np.expand_dims(crop, axis=0)\n",
    "                pred = model.predict(crop, verbose=2)\n",
    "                pred = pred.reshape((img_h, img_h, 1)).astype(np.float64)\n",
    "                #pred = np.argmax(pred, axis=2)\n",
    "                #print(pred.shape)\n",
    "                #pred = np.array(pred)\n",
    "                \n",
    "                pred[pred >= 0.5] = 1\n",
    "                pred[pred < 0.5] = 0\n",
    "                pred = pred * 255\n",
    "                '''\n",
    "                for a in range(img_h):\n",
    "                    for b in range(img_h):\n",
    "                        if pred[a, b] == 0.:\n",
    "                            temp[a, b, :] = [223, 223, 223]\n",
    "                        elif pred[a, b] == 1.:\n",
    "                            temp[a, b, :] = [255, 204, 163]\n",
    "                        else:\n",
    "                            print('Unknown type:', pred[a, b])\n",
    "                '''\n",
    "                mask_whole[h_begin:h_begin + img_h, w_begin:w_begin + img_w] \\\n",
    "                    = pred\n",
    "                # + mask_whole[i * stride:i * stride + image_size, j * stride:j * stride + image_size, :]\n",
    "        cv2.imwrite(predict_path + path1, mask_whole[0:h, 0:w])\n",
    "        #print('Done!')\n",
    "        \n",
    "predict_z(fileDir, preDir)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc80ac-f525-4cf2-bb24-390031a786c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e96416-8136-4a08-8517-4f5d532cfd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
